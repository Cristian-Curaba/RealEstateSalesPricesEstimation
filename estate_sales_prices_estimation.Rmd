---
title: "Estimation of Iranian Real Estate Units Sales Prices"
author: "C. Curaba, Y. Martínez Jiménez, E. Stefanel"
date: "2023-01-31"
output:
  html_document:
    toc: yes
    toc_depth: '3'
    df_print: paged
editor_options: 
  markdown: 
    wrap: 72
---

```{r setup, include=FALSE}
# Set chunks properties
knitr::opts_chunk$set(
  eval=TRUE, echo=FALSE, warning=FALSE,
  message=FALSE, fig.align='center')

# Import needed libraries for the project
library('dplyr')        # Base R package for data manipulation
library('openxlsx')     # Allows reading Excel files from URL
library('ggplot2')      # Allows plotting advanced graphs
library('AER')          # Applied Econometrics package
library('MASS')         # Includes useful functions for linear models evaluation
library('car')          # Companion to Applied Regression
library('boot')         # Makes Bootstrap Functions available
library('randomForest') # A rather explanatory name for a package
```


## Assignment statement

The [**Residential Building Data
Set**](https://archive.ics.uci.edu/ml/datasets/Residential+Building+Data+Set)
has been donated by Ph.D. Mohammad H. Rafiei from The Ohio State
University, Columbus, and includes $372$ observations about real estate
single-family residential apartments in Tehran, Iran. In particular, for
each apartment are reported $8$ project physical and financial variables
and $19$ economic variables and indices. This $19$ features are reported
in $5$ time lags[^1], leading a total of $103$ possible explanatory
variables for the two output variables, that are `ConstructionCosts` and
`SalesPrices`.

[^1]: The number of *time resolutions* before the start of the project.

The aim of the present project is to build an efficient pipeline to
estimate the `SalesPrices`, observing the explanatory variables.
Following the Occam's razor principle (*pluralitas non est ponenda sine
necessitate*, "plurality should not be posited without necessity"),
our aim is to get an easily interpretable model, rather than a very
complex one but difficult to understand. In order to do that, we start
by analyzing the dataset and performing some feature engineering. We
then study some possible statistical model for the estimation. Finally,
we conclude by summarizing the results and proposing some possible
future improvement.


## Data analysis

The first thing we do after importing the dataset from the original
repository is to rename the columns in a more friendly and informative
way. This is done by following the description of the features in the
`Description` sheet of the Excel file from the
`https://archive.ics.uci.edu` archive.

```{r dataset_import}
# Read the Constructions data from `archive.ics.uci.edu`
Constructions <- read.xlsx(
  'https://archive.ics.uci.edu/ml/machine-learning-databases/00437/Residential-Building-Data-Set.xlsx',
  sheet=1,
  startRow=2)

# Rename columns according to the descriptions in the Excel file
colnames(Constructions)[1:4] <- c("StartYear", "StartQuarter", "CompletionYear", "CompletionQuarter")
colnames(Constructions)[5:12] <- c("BuildingZIPCode", "BuildingFloorArea", "LotArea", "TotEstConstructionCost", "EstConstructionCost", "EstConstructionCostBaseYear", "ConstructionDuration", "PriceAtBeginning")

for (lag in 1:5) {
  colnames(Constructions)[(13+19*(lag-1)):(13+19*lag-1)] <- paste(
    c("BuildingPermitsNo", "BSI", "WPI", "BuildingPermitsFloorArea", "CumulativeLiquidity", "PrivateSectorInvestment", "LandPriceIndexBaseYear", "LoansExtendedNo", "LoansExtendedAmount", "InterestRate", "ConstructionCostAtCompletion", "ConstructionCostAtBeginning", "OfficialExcangeRateUSD", "StreetMarketExcangeRateUSD", "CPIBaseYear", "CPIFornituresBaseYear", "StockMarketIndex", "CityPopulation", "GoldPriceOnce"),
    lag, sep='_')
    rm(lag)
}

colnames(Constructions)[108:109] <- c("SalesPrices", "ConstructionCosts")

# Remove the `ConstructionCosts` column from the dataframe
Constructions <- Constructions %>% 
  dplyr::select(-c("ConstructionCosts"))

head(Constructions)
```

We are now going to list all the dataframe columns and explain their
meanings:

-   `StartYear`, `StartQuarter`, `CompletionYear` and
    `CompletionQuarter` are temporal references about the Building
    project. Only the last two digits of the years are reported, so we
    will add $1300$ to each year (keeping in mind that dates are
    referring to the Persian calendar);
-   `BuildingZIPCode` is the ZIP Code where the Building is located,.
    `BuildingFloorArea` and `LotArea` are surfaces values, measured in
    square meters, referring to the construction lot.
    `TotEstConstructionCost`, `EstConstructionCost` and
    `EstConstructionCostBaseYear` are total construction cost, in ten
    thousands Rial (Iranian currency), actual and estimated at the
    beginning of the project (the last is also adjusted to inflation at
    the year $1383$). `ConstructionDuration` is the duration, in
    quarters, of the constructions, and `PriceAtBeginning` is the
    specific square root price before the construction start. This are
    Building-specific features that do not change during the
    construction period;
-   There is then a set of $19$ features that are measured at five
    regular intervals from the construction starting date and the
    completion date. Each of them will then be present in the dataframe
    for a total of five times. `BuildingPermitsNo` is the number of
    building permits issued, `BSI` and `WPI` are Building Services Index
    and Wholesale Price Index, `BuildingPermitsFloorArea` is the total
    floor areas, in square meters, of building permits issued by the
    city. `CumulativeLiquidity` represents, in ten milions of Rial, how
    rapidly different types of assets can be changed to cash,
    `PrivateSectorInvestment` is the sector investment in new buildings,
    `LandPriceIndexBaseYear` is the square root price in year $1383$.
    The columns `LoansExtendedNo`, `LoansExtendedAmount` and
    `InterestRate` refers to extended loans during the lag.
    `ConstructionCostAtCompletion` and `ConstructionCostAtBeginning` are
    average construction cost of buildings by private sector at the time
    of completion and beginning of construction.
    `OfficialExcangeRateUSD` and `StreetMarketExcangeRateUSD` are
    official and nonofficial (street market) exchange rate with respect
    to dollars. `CPIBaseYear` and `CPIFornituresBaseYear` are
    respectively total Consumer Price Index and CPI only related to
    fornitures. `StockMarketIndex` represent the payback condition of
    investment in stock market, `CityPopulation` the population of the
    city and `GoldPriceOnce` gold price, in Rial, per ounce;
-   `SalesPrices` and `ConstructionCosts` are the two response
    variables, indicating the final selling price of the estate and
    total construction costs, respectively. Both measures use $10000$
    Rial as measure unit.

Now that we understand what the individual columns refer to, we can
analyze which variables are qualitative (ordered or not), and which are
quantitative (discrete or continuous).

The first thing that we note is that there are no qualitative columns in
the dataset. The only columns that could be interpreted as factor column
are the `StartQuarter` and `CompletionQuarter` ones, where only value
from $1$ to $4$ are possible, and `BuildingZIPCode` with values from $1$
to $20$. All the other variables are continuous quantitative measures.

```{r factor_columns}
# Treat `BuildingZIPCode` variable as factor
# Constructions$BuildingZIPCode <- as.factor(Constructions$BuildingZIPCode)

# We do not compute the same operation on `StartQuarter` and `CompletionQuarter`
# columns since, as we will see in a couple of chunks, we will drop that
# specific features.
```

Next, we proceed to check whether any records contain any missing
columns. If so, we will decide how to handle it.

```{r missing_values}
column_with_missing_values = c()

for (column in colnames(Constructions)) {
  missing_values <- Constructions %>% 
    dplyr::select(column) %>% 
    is.na() %>% 
    sum()
  
  if (missing_values > 0) {
    column_with_missing_values <- c(column_with_missing_values, column)
  }
}

if (length(column_with_missing_values) > 0) {
  print("There are columns with missing values!")
} else
{
  print("There are no columns with missing values!")
}

rm(column, missing_values, column_with_missing_values)
```

Fortunately, there are no columns where missing values occur. This can
be because a preventive data cleaning has been done by the person who
uploaded the dataframe on the public repository, or because the data
collection method was meticulous and error-free. We lean toward the
first of the two options.

We note that we could implode years and quarters information into a
single value, one for starting date and one for completion date. This is
done by simply adding $.25$ to each year value for each ended quarter of
the current year at the start (or completion) of the project. In this
way, the first quarter of the $x$ year will be $x.00$, the second
quarter will be $x.25$, and so on.

```{r feature_engineering}
# Explode the Years (abbreviated to the last two digits in the original dataframe) and add .25 year per Quarter
Constructions$StartYear <- ( Constructions$StartYear + 1300 ) + .25 * ( Constructions$StartQuarter - 1 )
Constructions$CompletionYear <- ( Constructions$CompletionYear + 1300 ) + .25 * ( Constructions$CompletionQuarter - 1 )

# Remove the `StartQuarter` and `CompletionQuarter` from the dataframe
Constructions <- Constructions %>% 
  dplyr::select(-c("StartQuarter", "CompletionQuarter"))

# Write final CSV file to filesystem
write.csv(Constructions, "./data/Constructions.csv" )
```

We now analyze the distribution of the response variable in the dataset.
Plotting the histogram, and analyzing mean and variance can help us.

```{r salesprices_hist}
# Print `SalesPrices` data summary
summary(Constructions$SalesPrices)

# Plot `SalesPrices` histogram
Constructions %>%
  ggplot(aes(x=SalesPrices)) + 
  geom_histogram() +
  geom_vline(aes(xintercept=mean(SalesPrices),color='mean'), linetype="dashed", size=.5) +
  geom_vline(aes(xintercept=median(SalesPrices),color='median'), linetype="dashed", size=.5) +
  scale_color_manual(name = "Statistics", values = c(mean = "red", median = "blue")) +
  labs(
    title='Sales prices histogram plot',
    subtitle='Distribution of Sales prices in the Constructions dataframe',
    caption="Data from archive.ics.uci.edu",
    x=expression("Sales prices (" %*% ~ 10^4 ~ "Rial)" ),
    y='Count'
  ) +
  theme_classic()
```

The distribution of `SalesPrices` values in the dataframe is highly
skewed towards left. We can try to plot the same histogram, but applying
a logarithmic scale to the vector of values.

```{r salesprices_log_hist}
# Plot `SalesPrices` logarithmic histogram
Constructions %>%
  ggplot(aes(x=SalesPrices)) +
  geom_histogram() +
  scale_x_log10() +
  geom_vline(aes(xintercept=mean(SalesPrices),color='mean'), linetype="dashed", size=.5) +
  geom_vline(aes(xintercept=median(SalesPrices),color='median'), linetype="dashed", size=.5) +
  scale_color_manual(name = "Statistics", values = c(mean = "red", median = "blue")) +
  labs(
    title='Sales prices logaritmic histogram plot',
    subtitle='Distribution of Sales prices in the Constructions dataframe',
    caption="Data from archive.ics.uci.edu",
    x=expression("Sales prices (" %*% ~ 10^4 ~ "Rial)" ),
    y='Count'
  ) +
  theme_classic()
```

We see that the distribution of the log-scaled `SalesPrices` can somehow
remember a normal distribution.

It is also very useful to study possible correlations in the dataset
between the response variable and the independent variables.

To do so, let's analyze the top three positively and top three
negatively correlated columns with `SalesPrices`, with relative
covariances:

```{r column_correlations}
cor <- cor(Constructions[ , colnames(Constructions) != "SalesPrices"], Constructions$SalesPrices)

head(cor[order(cor[,1], decreasing = TRUE), ], n=3)
tail(cor[order(cor[,1], decreasing = TRUE), ], n=3)
```

It is straightforward to note that `PriceAtBeginning` has a really high
correlation with the response variable. Recall that `PriceAtBeginning`
column is the price of the building unit at the beginning of the
project. The relation is evident if we draw the scatter plot of the two
variables.

```{r column_correlations_plot}
# Plot the scatter plot between `PriceAtBeginning` and `SalesPrices`
Constructions %>%
  ggplot(aes(x=PriceAtBeginning, y=SalesPrices)) + 
  geom_point() +
#  scale_x_log10() +
#  scale_y_log10() +
  stat_smooth(method="lm",formula=y~x, se=FALSE) +
  labs(
    title='PriceAtBeginning-SalesPrices correlation plot',
    subtitle='Correlation between Price at construction begin and Sales prices in the Constructions dataframe',
    caption="Data from archive.ics.uci.edu",
    x=expression("Price at construction begin (" %*% ~ 10^4 ~ "Rial)" ),
    y=expression("Sales prices (" %*% ~ 10^4 ~ "Rial)" )
  ) +
  theme_classic()
```

As for negative correlations, `BuildingZIPCode` reports a $-0.44$ index
of correlation. This may suggest that high-ZIP code buildings report
minor final `SalesPrices`, adjusted for the others variables.

```{r zipcodes_boxplot}
Constructions %>%
  ggplot(aes(x=as.factor(BuildingZIPCode), y=SalesPrices)) + 
  geom_boxplot() +
  labs(
    title='SalesPrices by BuildingZIPCode',
    caption="Data from archive.ics.uci.edu",
    x=expression("BuildingZIPCode" ),
    y=expression("Sales prices (" %*% ~ 10^4 ~ "Rial)" )
  ) +
  theme_classic()
```

With a visual inspection of the relation, we note that some ZIP codes
are more influential than others in predicting the `SalesPrices`
response. We decide to summary `BuildingZIPCode` information into four
main groups: ZIP Codes number $1$, $3$, $6$ and other ZIP Codes (named
$21$).

```{r zipcodes_featuring}
# Functions that returns the original value is ZIPCode equals 1, 3 or 6,
# and 21 otherwise.
newBuildingZIPCode <- function(ZIPCode) {
    if (ZIPCode %in% c(1,3,6)) {
        ZIPCode
    }
    else {
        21
    }
}

# Apply the `newBuildingZIPCode` function to each `BuildingZIPCode` observation
Constructions$BuildingZIPCode <- sapply(Constructions$BuildingZIPCode, newBuildingZIPCode)
Constructions$BuildingZIPCode <- as.factor(Constructions$BuildingZIPCode)

rm(newBuildingZIPCode)

# View the dataset head after the features ingeneering part
head(Constructions)
```


## Statistical models

Since there are a lot of explanatory variables, we must drop lot of them
to avoid multicollinearity and over-complexity.

As baseline model, we are going to consider a linear model with
`PriceAtBeginning` predictor variable.

```{r linearmodel_baseline}
# Attach the `Constructions` dataframe to the R search path
attach(Constructions)

# Train a simple linear model and print its summary and residuals qqplot
baseline_linear<-lm(SalesPrices~PriceAtBeginning)
summary(baseline_linear)
qqnorm(baseline_linear$residuals)
```

We can see that the homoscedasticity hypothesis is not satisfied. It may
be a good idea to consider a Generalized Linear Model of the `poisson`
family:

```{r poisson_baseline}
poisson_baseline<-glm(SalesPrices~PriceAtBeginning, family=poisson())
summary(poisson_baseline)

dispersiontest(poisson_baseline) #Checking overdisperion
```

Since there is overdispersion and high residuals, let's try the negative
binomial model with `log` and `identity` link function.

```{r NegBin_baseline}
NegBin_baseline_log<-glm.nb(SalesPrices~PriceAtBeginning, link="log", data=Constructions)
NegBin_baseline_id<-glm.nb(SalesPrices~PriceAtBeginning, link="identity", data=Constructions)
#summary(NegBin_baseline_log)
#summary(NegBin_baseline_id)

print("Negative Binomial baseline model (`log` link)")
print(c("Residuals:", summary(NegBin_baseline_log$residuals)))
print(c("AIC:", NegBin_baseline_log$aic))

print("-----")

print("Negative Binomial baseline model (`identity` link)")
print(c("Residuals:", summary(NegBin_baseline_id$residuals)))
print(c("AIC:", NegBin_baseline_id$aic))

```

Confronting *Akaike information criterion* (AIC) and Residuals, we can
easily choose the negative binomial model with `identity` link function.
We also tried other standard generalized linear model (with different
link functions) without reporting here: they all performs worse
(comparing AIC values) then the negative binomial with identity as link
function. We would like to mention the gamma family with identity as
link function: it performs nearly the same. So we decided to consider
both models and then choose the best. To find the best features we
consider an automatic stepwise algorithm to choose a model by AIC. Since
we may avoid some rough collinearity, we decided to feed the algorithm
with an upper model containing just some of the $19$ features that are
measured through time. With a brief analysis (omitted here for the sake
of brevity) we decided to consider just the indexes at the building
completion (marked as '\_5' in the dataset).

```{r}
##Building indexes5 containing only the environment indexes at completion time
ColNames<-colnames(Constructions)
j=1;
indexes5<-0;
for (i in 1:length(ColNames)){
  if(grepl('_5', ColNames[i])){indexes5[j]<-i; j<-j+1;}
}
rm(j)
rm(i)
```

Now let's build the two best model (with Gamma and Negative Binomial)
based on AIC using the stepwise algorithm.

```{r NegBin}
all_index_noZIP<-c(1,2,4:10,indexes5)
max_mod_NegBin <-glm.nb(as.formula(paste("SalesPrices~factor(BuildingZIPCode)+", paste(ColNames[all_index_noZIP], collapse="+"))), data=Constructions, link="identity") #upper model

#termplot(max_mod_NegBin, partial.resid = TRUE) #Checking if we need to rescale some feature (omitted for brevity); they all present smooth straight lines so no rescaling o GAMs needed.

min_mod_NegBin <-glm.nb(SalesPrices~ PriceAtBeginning, link="identity") #lower model
```

```{r fitting neg bin, results="hide"}
fit_best_NegBin<-step(min_mod_NegBin, scope=list(lower=min_mod_NegBin, upper=max_mod_NegBin), direction = "both") #Best fitting 
```

```{r summaries NegBin }
vif(fit_best_NegBin) #Checking collinearity
summary(fit_best_NegBin)
```

A future improvement can be done eliminating collinearity to comprehend
the real impact of each feature.

Let's make the same work with the gamma family and just see the results.

```{r gamma_model, results="hide"}
max_mod_Gamma <-glm(as.formula(paste("SalesPrices~factor(BuildingZIPCode)+", paste(ColNames[all_index_noZIP], collapse="+"))), data=Constructions, family = Gamma(link="identity"))
min_mod_Gamma <-glm(SalesPrices~ PriceAtBeginning, family=Gamma(link="identity"))

fit_best_Gamma<-step(min_mod_Gamma, scope=list(lower=min_mod_Gamma, upper=max_mod_Gamma), direction = "both")
```

```{r summary gamma model}
summary(fit_best_Gamma)
#plot(fit_best_Gamma)
```

By confronting AIC values, we can easily say that the best model is the
gamma with identity link function.

At cost of interpretability, we can also build the model using all the
environment indexes and see if the model improves.

```{r fitting gamma model all, results="hide"}
max_mod_Gamma_all <-glm(SalesPrices~., family=Gamma(link="identity"),data=Constructions)

fit_best_Gamma_all<-step(min_mod_Gamma, scope=list(lower=min_mod_Gamma, upper=max_mod_Gamma_all), direction = "both")
```
```{r summury gamma all}
summary(fit_best_Gamma_all)
```

The resulting model is really complex but better in predicting. We perform a
10-cross validation to assess our models estimating the MSE errors
(and checking if overfitting occurs).

```{r 10-CV., echo=TRUE}
#MSE and MSE adjusted
set.seed(234)
cv.glm(Constructions, fit_best_NegBin, K=10)$delta #NegBin
cv.glm(Constructions, fit_best_Gamma,K=10)$delta #Gamma
cv.glm(Constructions, fit_best_Gamma_all, K=10)$delta #Gamma_all

mean((SalesPrices-predict(fit_best_Gamma_all))^2) #Gamma_all on the dataset
```

We obtain that the last model (gamma with all features) is a bit overfitting
the data but is still performing better.
Let's also compare them with a random forest model (keeping all the features).

```{r}
predictor_randomForest<-randomForest(SalesPrices~., data=Constructions)
MSE_randomForest<-mean((SalesPrices-predict(predictor_randomForest))^2)
print(c("MSE:", MSE_randomForest))
varImpPlot(predictor_randomForest)
```

We can see that the MSE error is way bigger than the last one. A brief look at
the variable importance plot may be informative.


## Conclusions

At the beginning of the data analysis we used as a base model the linear model
with the predictor variable `PriceAtBeginning`. We choose this variable because
in the study of the correlation between the response variable and the predictor
variables, `PriceAtBeginning` is the predictor variable with the highest positive
correlation value ($0.976$). Then we saw that the hypothesis of homoscedasticity
was not fulfilled, we decided to consider the Poisson model (which fails because
it has overdispersion), the negative binomial with log link function, and the
negative binomial model with identity link function, since they are the most
appropriate for our response variable.

From the study conducted on the correlation between the response variable and
the predictor variables we can also highlight that the highest negative value
($-0.435$) corresponds to the variable `BuildingZIPCode`, this means that
the selling price of the buildings is higher when the ZIP Codes is lower.
So as some ZIP codes are more influential than others,
we decided to group them into 4 different groups.

After a simple comparison of the *Akaike Information Criterion* values, we choose
the negative binomial model with the identity link function as it has the lowest
AIC value. We proceeded to compare this model with a gamma model with identity
link function as they are similar.

To avoid collinearity and after a brief analysis, we consider a new model whose
variables are:

*   of the 19 characteristics that are measured over time, those that measure
    the period of building completion;
*   The transformation of the `BuildingZIPCode` variable, in which we
    divided the original ZIP codes into four main groups.

In this way, we constructed the two best-fitting models and compared them:

In the case of the negative binomial model, when checking the collinearity we
see that there are variables with high VIF values and therefore the model could
be improved by eliminating the collinearity. When building these model, if we
focus on the variables with low VIF value (less than 5), such as
`ConstructionDuration` or `InteresRate_5`, and look at their p-values, we can
see that these predictor variables are statistically significant at the $0.05$
significance level and thus would be quite fundamental for a good fit.
The only exception is the variable `TotEstConstructionCost`that has a low VIF
but his pvalue is not significance at level $0.05$ level.

In the case of the gamma model, if we look at the p-values of the variables
mentioned above, we can see that they are values well below $0.05$ and are
therefore even more significant.

After developing the models and comparing the AIC values, we conclude that the
best model is the gamma model with the identity link function.

Finally, to make sure we made the best choice, we compared the model we
were working with (whose predictor variables were modified) with the
gamma model with all the original variables (all the environmental indices) and
for this model we obtained a lower AIC value, therefore it is better.

Through the 10-cross validation and comparing by mean squared error the three
models, we obtained again that the lowest RMSE values are those of the gamma model
with all variables. Even when comparing this model with a Random Forest
(using all features), we see that, indeed, the forest model has a higher MSE error.

Therefore, to estimate the `SalesPrices` variable in this dataset,
the most appropriate model is the gamma model including all explanatory variables.
